
from tokenizers.models import BPE
from tokenizers import Tokenizer
from tokenizers.decoders import ByteLevel as ByteLevelDecoder
from tokenizers.normalizers import Sequence, Lowercase
from tokenizers.pre_tokenizers import ByteLevel
from tokenizers.trainers import BpeTrainer
from transformers import PreTrainedTokenizerFast, GPT2TokenizerFast
import tensorflow as tf
import numpy as np



tokenizer_path  = "noomo"

##########################################################################################

tokenizer = Tokenizer(BPE())
tokenizer.normalizer = Sequence([Lowercase()])
tokenizer.pre_tokenizer = ByteLevel()
tokenizer.decoder = ByteLevelDecoder()

trainer = BpeTrainer(vocab_size=50000, initial_alphabet=ByteLevel.alphabet(), min_frequency=1,
                    special_tokens=["<pad>", "<s>", "</s>", "<unk>", "<mask>"]
                    )

tokenizer.train(["data/synthetic-data.txt"], trainer)

tokenizer.add_tokens([
    "do",
    "ing",
    "are",
    "as",
    "was",
    "es",
    "er",
    "were",
    "will",
    "all",
    "is",
    "his",
    "this",
    "learn",
    "hear",
    "wear",
    "to",
    "ed",
    "look",
    "watch",
    "work",
    "help",
    "need",
    "open",
    "visit",
    "call",
    "run",
    "teach",
    "see",
    "they",
    "where",
    "there",
    "here",
    "no",
    "nope",
    "not",
    "for",
    "some",
    "every",
    "each",
    "any",
    "still",
    "fill",
    "ill",
    "want",
    "be",
    "or",
    "so",
    "the",
    "that",
    "this",
    "its",
    "an",
    "should",
    "would",
    "could",
    "may",
    "say",
    "might",
    "fix",
    "post",
    "pre",
    "pro",
    "put",
    #"ation",
    #"ession",
    "too",
    "also",
    "but",
    "and",
    "end",
    "never",
    "ever",
    "inter",
    "ion", #
    "tial",
    "com",
    "mit",
    "ional",#
    "ess",
    "istic",
    "on",
    "gather",
    "gether",
    "at",
    "with",
    "very",
    "less",
    "most",
    "must",
    "more",
    "she",
    "ago",
    "about",
    "till",
    "from",
    "been",
    "ish",
    "able",
    "ence",
    "ible",
    "them",
    "than",
    "then",
    "just",
    "ify",
    "such",
    "much",
    "ment",
    "self",
    "true",
    "false",
    "off",
    "body",
    "again",
    "ness",
    "only",
    "down",
    "back",
    "shall",
    "how",
    "well",
    "good",
    "which",
    "great",
    "while",
    "what",
    "when",
    "over",
    "high",
    "thing",
    "mrs",
    "out",
    "next",
    "time",
    "past",
    "half",
    "among",
    "between",
    "soon",
    "above",
    "ical",
    "other",

    "few",
    "always",
    "cause",

    "did",
    "can",
    "ally",
    "elly",
    "ask",
    "mask",
    "task",
    "her",
    "me",
    "by",
    "my",
    "up",
    "you",
    "your",
    "young",
    "we",
    "ly",
    "us",
    "use",
    "usual",
    "who",
    "get",
    "take",
    "make",
    "know",
    "through",
    "else",

    "hear",
    "wish",
    "enough",
    "have",
    "love",
    "pair",
    "friend",
    "idea",
    "life",
    "fear",
    "though",
    "avoid",
    "own",
    "home",
    "speak",
    "sweet",
    "had",
    "house",
    "knew",
    "last",
    "without",
    "seem",
    "door",
    "valid",
    "describ",
    "walk",
    "meet",
    "met",
    "mean",
    "real",
    "listen",
    "certain",
    "past",
    "new",
    "spent",
    "continu",
    "left",
    "right",
    "return",
    "thought",
    "join",
    "answer",
    "said",
    "now",
    "play",
    "like",
    "pass",
    "many",
    "one",
    "day",
    "year",
    "place",
    "course",
    "well",
    "word",
    "decide",
    "discover",
    "upon",
    "simple",
    "observ",
    "require",
    "add",
    "once",
    "sure",
    "appear",
    "both",
    "hope",
    "indeed",
    "honest",
    "truth",
    "letter",
    "spend",
    "dear",
    "side",
    "declare",
    "reply",
    "told",
    "fact",
    "same",
    "fother",
    "mother",
    "write",
    "wrote",
    "farther",
    "disapoint",
    "oppos",
    "topic",
    "except",
    "confident",
    "power",
    "close",
    "neither",
    "allow",
    "oppos",
    "object",
    "compan",
    "discuss",
    "author",
    "resolv",
    "present",
    "long",
    "took",
    "toward",
    "little",
    "yet",
    "heart",
    "old",
    "lady",
    "subject",
    "kind",
    "recent",
    "act",
    "quiet",
    "even",
    "already",
    "way",
    "turn",
    "occurr",
    "merely",
    "possib",
    "produc",
    "evening",
    "morning",
    "suppos",
    "person",
    "determin",
    "perfect",
    "general",
    "kind",
    "perhaps",
    "opinion",
    "begin",
    "strong",
    "guess",
    "hour",
    "pleasure",
    "easy",
    "man",
    "woman",
    "whom",
    "pleas",
    "inform",
    "child",
    "family",
    "their",
    "tell",
    "manner",
    "spirit",

    #"weymouth",
    #"windsor",
    "harriet",
    "emma",
    "elton",
    "woodhouse",
    "goddard",
    "hartfield",
    "knightley",
    "perry",
    "churchill",
    "frank",
    "weston",
    "campbell",
    "jane",
    "fairfax",
    "taylor",
    "cole",
    "colonel",
    "bates",
    "john",
    "randalls",
    "highbury",
    "william",
    "isabella",
    "smith",
    "martin",
    "robert",
    "marry",
    "dixon",
    "enscombe",

    "stay",
    "cold",
    "deal",
    "voice",
    "wonder",
    "reason",
    "rather",
    "however",
    "superior",
    "natur",
    "chance",
    "engage",
    "agree",
    "miss",
    "beyond",
    "another",
    "set",
    "sit",
    "doubt",
    "fancy",
    "forward",
    "hand",
    "mind",
    "week",
    "follow",
    "remember",
    "cool",
    "parent",
    "live",
    "respect",
    "gave",
    "into",
    "read",
    "dinner",
    "away",
    "bring",
    "stand",
    "pretty",
    "point",
    "under",
    "question",
    "children",
    "account",
    "consider",
    "brother",
    "world",
    "state",
    "people",
    "better",
    "feel",
    "comfort",
    "having",
    "dare",
    "wife",
    "found",
    "daughter",
    "today",
    "tomorrow",
    "affect",
    "went",
    "mention",
    "ought",
    "against",
    "far",
    "attention",
    "sorry",
    "felt",
    "first",
    "find",
    "small",
    "attach",
    "temper",
    "happy",
    "party",
    "enjoy",
    "satisf",
    "those",
    "relat",

    "circumstance",
    "neighbour",
    "interest",
    "conversation",
    "explan",
    "instrument",
    "acquaintance",
    "pianoforte",
    "appearance",
    "reflect",
    "eager",
    "depend",
    "suspect",
    "believ",
    "acknowledg",
    "happiness",
    "absolut",
    "understand",
    "character",
    ])

fast_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object = tokenizer,
    bos_token = "<s>",
    eos_token = "</s>",
    unk_token = "<unk>",
    pad_token = "<pad>",
    mask_token = "<mask>"
)

fast_tokenizer.save_pretrained(tokenizer_path)

##########################################################################################

tokenizer_gpt = GPT2TokenizerFast.from_pretrained(tokenizer_path, local_files_only=True)

def statistic(tokenizer_gpt: GPT2TokenizerFast):
    print(f"tokenizer_gpt.config: vocab.sz={len(tokenizer_gpt.get_vocab())},",
        f"pad_token_id={tokenizer_gpt.pad_token_id},",
        f"bos_token_id={tokenizer_gpt.bos_token_id},",
        f"eos_token_id={tokenizer_gpt.eos_token_id}",
        )

##########################################################################################

def print_tokenization(prompt: str):
    input_ids = tokenizer_gpt(prompt, add_special_tokens=False, padding=False, return_tensors="np")
    input_ids = input_ids["input_ids"]
    input_ids = input_ids[0]

    #print(input_ids)
    print(tokenizer_gpt.convert_ids_to_tokens(input_ids))
    print(tokenizer_gpt.decode(input_ids, skip_special_tokens=False))

print_tokenization("Learning learns learned hears is hearing")

print_tokenization("Do doing does teach teacher teaching")

print_tokenization("Wear wears wearing his this are wanting wanted wants")

print_tokenization("All is as was running were will")

print_tokenization("Here where there no nope not therefore anywhere still fill ill")

print_tokenization(
    "be being or so the that this its an should would could may say might fix post pre pro put ation ession too also but and end")

print_tokenization("postfix prefix international putting forever somewhere never profession professional")

print_tokenization("come become commit comes common cannot can't sooner")

print_tokenization("gather gathering gathered together more she because didn't")

print_tokenization("Ask ask task mask tasking masking")

print_tokenization("you young your yours we were mostly")

print_tokenization("us use used uses using usual usually known knows whenever everyday illness seemingly")

statistic(tokenizer_gpt)
